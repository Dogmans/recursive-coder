# Copy this to .env and fill in values

# ── Provider ────────────────────────────────────────────────
# Which model endpoint to use.  One of:  github | huggingface | openai | local
SMOL_PROVIDER=github

# ── Model ID ────────────────────────────────────────────────
# The model identifier — meaning depends on the provider:
#   github       →  catalog name, e.g. gpt-4o, o3-mini, DeepSeek-R1
#   huggingface  →  HF model ID, e.g. Qwen/Qwen2.5-Coder-32B-Instruct
#   openai       →  OpenAI model name, e.g. gpt-4o, o3-mini
#   local        →  HF model ID to download, e.g. microsoft/phi-3-mini-4k-instruct
SMOL_MODEL_ID=gpt-4o

# ── Tokens / API keys (set the one(s) you need) ────────────
# GitHub personal access token   — for provider=github
GITHUB_TOKEN=ghp_your_token_here
# Hugging Face token              — for provider=huggingface
HF_TOKEN=hf_your_token_here
# OpenAI API key                   — for provider=openai
OPENAI_API_KEY=sk-your_key_here

# ── Optional overrides ──────────────────────────────────────
# Custom base URL (overrides the provider default)
# SMOL_API_BASE=https://models.inference.ai.azure.com

# Max agent reasoning steps
SMOL_MAX_STEPS=30

# Hugging Face cache directory (models download here when local)
# HF_HOME=E:\.huggingface_cache

# Local-only settings
# SMOL_QUANTIZE=true
